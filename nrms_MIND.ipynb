{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjxsvhHfriRe"
      },
      "source": [
        "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
        "\n",
        "<i>Licensed under the MIT License.</i>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rT-IG48HriRi"
      },
      "source": [
        "# NRMS: Neural News Recommendation with Multi-Head Self-Attention\n",
        "NRMS \\[1\\] is a neural news recommendation approach with multi-head selfattention. The core of NRMS is a news encoder and a user encoder. In the newsencoder, a multi-head self-attentions is used to learn news representations from news titles by modeling the interactions between words. In the user encoder, we learn representations of users from their browsed news and use multihead self-attention to capture the relatedness between the news. Besides, we apply additive\n",
        "attention to learn more informative news and user representations by selecting important words and news.\n",
        "\n",
        "## Properties of NRMS:\n",
        "- NRMS is a content-based neural news recommendation approach.\n",
        "- It uses multi-self attention to learn news representations by modeling the iteractions between words and learn user representations by capturing the relationship between user browsed news.\n",
        "- NRMS uses additive attentions to learn informative news and user representations by selecting important words and news.\n",
        "\n",
        "## Data format:\n",
        "For quicker training and evaluaiton, we sample MINDdemo dataset of 5k users from [MIND small dataset](https://msnews.github.io/). The MINDdemo dataset has the same file format as MINDsmall and MINDlarge. If you want to try experiments on MINDsmall and MINDlarge, please change the dowload source. Select the MIND_type parameter from ['large', 'small', 'demo'] to choose dataset.\n",
        " \n",
        "**MINDdemo_train** is used for training, and **MINDdemo_dev** is used for evaluation. Training data and evaluation data are composed of a news file and a behaviors file. You can find more detailed data description in [MIND repo](https://github.com/msnews/msnews.github.io/blob/master/assets/doc/introduction.md)\n",
        "\n",
        "### news data\n",
        "This file contains news information including newsid, category, subcatgory, news title, news abstarct, news url and entities in news title, entities in news abstarct.\n",
        "One simple example: <br>\n",
        "\n",
        "`N46466\tlifestyle\tlifestyleroyals\tThe Brands Queen Elizabeth, Prince Charles, and Prince Philip Swear By\tShop the notebooks, jackets, and more that the royals can't live without.\thttps://www.msn.com/en-us/lifestyle/lifestyleroyals/the-brands-queen-elizabeth,-prince-charles,-and-prince-philip-swear-by/ss-AAGH0ET?ocid=chopendata\t[{\"Label\": \"Prince Philip, Duke of Edinburgh\", \"Type\": \"P\", \"WikidataId\": \"Q80976\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [48], \"SurfaceForms\": [\"Prince Philip\"]}, {\"Label\": \"Charles, Prince of Wales\", \"Type\": \"P\", \"WikidataId\": \"Q43274\", \"Confidence\": 1.0, \"OccurrenceOffsets\": [28], \"SurfaceForms\": [\"Prince Charles\"]}, {\"Label\": \"Elizabeth II\", \"Type\": \"P\", \"WikidataId\": \"Q9682\", \"Confidence\": 0.97, \"OccurrenceOffsets\": [11], \"SurfaceForms\": [\"Queen Elizabeth\"]}]\t[]`\n",
        "<br>\n",
        "\n",
        "In general, each line in data file represents information of one piece of news: <br>\n",
        "\n",
        "`[News ID] [Category] [Subcategory] [News Title] [News Abstrct] [News Url] [Entities in News Title] [Entities in News Abstract] ...`\n",
        "\n",
        "<br>\n",
        "\n",
        "We generate a word_dict file to transform words in news title to word indexes, and a embedding matrix is initted from pretrained glove embeddings.\n",
        "\n",
        "### behaviors data\n",
        "One simple example: <br>\n",
        "`1\tU82271\t11/11/2019 3:28:58 PM\tN3130 N11621 N12917 N4574 N12140 N9748\tN13390-0 N7180-0 N20785-0 N6937-0 N15776-0 N25810-0 N20820-0 N6885-0 N27294-0 N18835-0 N16945-0 N7410-0 N23967-0 N22679-0 N20532-0 N26651-0 N22078-0 N4098-0 N16473-0 N13841-0 N15660-0 N25787-0 N2315-0 N1615-0 N9087-0 N23880-0 N3600-0 N24479-0 N22882-0 N26308-0 N13594-0 N2220-0 N28356-0 N17083-0 N21415-0 N18671-0 N9440-0 N17759-0 N10861-0 N21830-0 N8064-0 N5675-0 N15037-0 N26154-0 N15368-1 N481-0 N3256-0 N20663-0 N23940-0 N7654-0 N10729-0 N7090-0 N23596-0 N15901-0 N16348-0 N13645-0 N8124-0 N20094-0 N27774-0 N23011-0 N14832-0 N15971-0 N27729-0 N2167-0 N11186-0 N18390-0 N21328-0 N10992-0 N20122-0 N1958-0 N2004-0 N26156-0 N17632-0 N26146-0 N17322-0 N18403-0 N17397-0 N18215-0 N14475-0 N9781-0 N17958-0 N3370-0 N1127-0 N15525-0 N12657-0 N10537-0 N18224-0`\n",
        "<br>\n",
        "\n",
        "In general, each line in data file represents one instance of an impression. The format is like: <br>\n",
        "\n",
        "`[Impression ID] [User ID] [Impression Time] [User Click History] [Impression News]`\n",
        "\n",
        "<br>\n",
        "\n",
        "User Click History is the user historical clicked news before Impression Time. Impression News is the displayed news in an impression, which format is:<br>\n",
        "\n",
        "`[News ID 1]-[label1] ... [News ID n]-[labeln]`\n",
        "\n",
        "<br>\n",
        "Label represents whether the news is clicked by the user. All information of news in User Click History and Impression News can be found in news data file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2z9qI_ariRo"
      },
      "source": [
        "## Global settings and imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scrapbook\n",
        "!pip install recommenders"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pIRK2N89rmkl",
        "outputId": "5e335dc5-efbf-4d39-b3be-2e60371f04e3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting scrapbook\n",
            "  Downloading scrapbook-0.5.0-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from scrapbook) (5.5.0)\n",
            "Collecting papermill\n",
            "  Downloading papermill-2.3.4-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from scrapbook) (6.0.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scrapbook) (1.3.5)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from scrapbook) (4.3.3)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (1.0.18)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->scrapbook) (57.4.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->scrapbook) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->scrapbook) (0.2.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (4.11.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (4.1.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (5.7.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (0.18.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->scrapbook) (21.4.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->scrapbook) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->scrapbook) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (2.23.0)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.4)\n",
            "Requirement already satisfied: nbformat>=5.1.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (5.4.0)\n",
            "Requirement already satisfied: tqdm>=4.32.2 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (4.64.0)\n",
            "Requirement already satisfied: nbclient>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (0.6.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (3.13)\n",
            "Collecting ansiwrap\n",
            "  Downloading ansiwrap-0.8.4-py2.py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (8.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from papermill->scrapbook) (7.1.2)\n",
            "Collecting jupyter-client>=6.1.5\n",
            "  Downloading jupyter_client-7.3.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 64.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from nbclient>=0.2.0->papermill->scrapbook) (1.5.5)\n",
            "Collecting traitlets>=4.2\n",
            "  Downloading traitlets-5.3.0-py3-none-any.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 70.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyzmq>=23.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (23.1.0)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client>=6.1.5->nbclient>=0.2.0->papermill->scrapbook) (4.10.0)\n",
            "Collecting tornado>=6.0\n",
            "  Downloading tornado-6.1-cp37-cp37m-manylinux2010_x86_64.whl (428 kB)\n",
            "\u001b[K     |████████████████████████████████| 428 kB 3.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.1.2->papermill->scrapbook) (2.15.3)\n",
            "Collecting textwrap3>=0.9.2\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->scrapbook) (0.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->papermill->scrapbook) (2022.6.15)\n",
            "Installing collected packages: traitlets, tornado, textwrap3, jupyter-client, ansiwrap, papermill, scrapbook\n",
            "  Attempting uninstall: traitlets\n",
            "    Found existing installation: traitlets 5.1.1\n",
            "    Uninstalling traitlets-5.1.1:\n",
            "      Successfully uninstalled traitlets-5.1.1\n",
            "  Attempting uninstall: tornado\n",
            "    Found existing installation: tornado 5.1.1\n",
            "    Uninstalling tornado-5.1.1:\n",
            "      Successfully uninstalled tornado-5.1.1\n",
            "  Attempting uninstall: jupyter-client\n",
            "    Found existing installation: jupyter-client 5.3.5\n",
            "    Uninstalling jupyter-client-5.3.5:\n",
            "      Successfully uninstalled jupyter-client-5.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires tornado~=5.1.0; python_version >= \"3.0\", but you have tornado 6.1 which is incompatible.\u001b[0m\n",
            "Successfully installed ansiwrap-0.8.4 jupyter-client-7.3.4 papermill-2.3.4 scrapbook-0.5.0 textwrap3-0.9.2 tornado-6.1 traitlets-5.3.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tornado"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting recommenders\n",
            "  Downloading recommenders-1.1.0-py3-none-manylinux1_x86_64.whl (335 kB)\n",
            "\u001b[K     |████████████████████████████████| 335 kB 34.5 MB/s \n",
            "\u001b[?25hCollecting memory-profiler<1,>=0.54.0\n",
            "  Downloading memory_profiler-0.60.0.tar.gz (38 kB)\n",
            "Collecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.3.tar.gz (10 kB)\n",
            "Requirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.2.3)\n",
            "Requirement already satisfied: nltk<4,>=3.4 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.7)\n",
            "Collecting cornac<2,>=1.1.2\n",
            "  Downloading cornac-1.14.2-cp37-cp37m-manylinux1_x86_64.whl (12.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.4 MB 27.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.21.6)\n",
            "Collecting lightfm<2,>=1.15\n",
            "  Downloading lightfm-1.16.tar.gz (310 kB)\n",
            "\u001b[K     |████████████████████████████████| 310 kB 68.6 MB/s \n",
            "\u001b[?25hCollecting pandera[strategies]>=0.6.5\n",
            "  Downloading pandera-0.9.0-py3-none-any.whl (197 kB)\n",
            "\u001b[K     |████████████████████████████████| 197 kB 66.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.3.5)\n",
            "Collecting scikit-surprise>=1.0.6\n",
            "  Downloading scikit-surprise-1.1.1.tar.gz (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 51.3 MB/s \n",
            "\u001b[?25hCollecting pyyaml<6,>=5.4.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 69.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2<3.1,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (4.64.0)\n",
            "Collecting bottleneck<2,>=1.2.1\n",
            "  Downloading Bottleneck-1.3.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_24_x86_64.whl (332 kB)\n",
            "\u001b[K     |████████████████████████████████| 332 kB 60.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.51.2)\n",
            "Requirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (0.11.2)\n",
            "Requirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders) (3.2.2)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.4.1)\n",
            "Collecting transformers<5,>=2.5.0\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 52.2 MB/s \n",
            "\u001b[?25hCollecting category-encoders<2,>=1.3.0\n",
            "  Downloading category_encoders-1.3.0-py2.py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<1.0.3,>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from recommenders) (1.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders) (2.23.0)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.5.2)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders) (0.10.2)\n",
            "Collecting powerlaw\n",
            "  Downloading powerlaw-1.5-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<3.1,>=2->recommenders) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders) (1.4.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib<4,>=2.2.2->recommenders) (4.1.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler<1,>=0.54.0->recommenders) (5.4.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (7.1.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders) (1.1.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders) (2022.1)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (6.0.1)\n",
            "Collecting typing-inspect>=0.6.0\n",
            "  Downloading typing_inspect-0.7.1-py3-none-any.whl (8.4 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.14.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (21.3)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders) (1.8.2)\n",
            "Collecting hypothesis>=5.41.1\n",
            "  Downloading hypothesis-6.48.2-py3-none-any.whl (387 kB)\n",
            "\u001b[K     |████████████████████████████████| 387 kB 71.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (21.4.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders) (2.4.0)\n",
            "Collecting exceptiongroup>=1.0.0rc8\n",
            "  Downloading exceptiongroup-1.0.0rc8-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->category-encoders<2,>=1.3.0->recommenders) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1.0.3,>=0.22.1->recommenders) (3.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (4.11.4)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 46.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders) (3.7.1)\n",
            "Collecting mypy-extensions>=0.3.0\n",
            "  Downloading mypy_extensions-0.4.3-py2.py3-none-any.whl (4.5 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders) (3.8.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders) (1.2.1)\n",
            "Building wheels for collected packages: lightfm, memory-profiler, retrying, scikit-surprise\n",
            "  Building wheel for lightfm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lightfm: filename=lightfm-1.16-cp37-cp37m-linux_x86_64.whl size=705366 sha256=6fcfa441bcc3aae61b5623805c4fdf4d35abf215cfc13eea2251fd3168a4c325\n",
            "  Stored in directory: /root/.cache/pip/wheels/f8/56/28/5772a3bd3413d65f03aa452190b00898b680b10028a1021914\n",
            "  Building wheel for memory-profiler (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for memory-profiler: filename=memory_profiler-0.60.0-py3-none-any.whl size=31284 sha256=1636cab3441e70ba79e9b4df392e06d015617e079d320f9fffbb3cffa18508c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/2b/fb/326e30d638c538e69a5eb0aa47f4223d979f502bbdb403950f\n",
            "  Building wheel for retrying (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for retrying: filename=retrying-1.3.3-py3-none-any.whl size=11447 sha256=3bf841d2ff9fa8b540e12c1248172d0d342762b03c79b2cd029448f93b408c5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/8d/8d/f6af3f7f9eea3553bc2fe6d53e4b287dad18b06a861ac56ddf\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.1-cp37-cp37m-linux_x86_64.whl size=1633716 sha256=ab48612890d101b971e826f6d709fc153660b0da48ecfde2beee18722a72c79b\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/44/74/b498c42be47b2406bd27994e16c5188e337c657025ab400c1c\n",
            "Successfully built lightfm memory-profiler retrying scikit-surprise\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyyaml, exceptiongroup, tokenizers, powerlaw, pandera, hypothesis, huggingface-hub, transformers, scikit-surprise, retrying, memory-profiler, lightfm, cornac, category-encoders, bottleneck, recommenders\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed bottleneck-1.3.4 category-encoders-1.3.0 cornac-1.14.2 exceptiongroup-1.0.0rc8 huggingface-hub-0.8.1 hypothesis-6.48.2 lightfm-1.16 memory-profiler-0.60.0 mypy-extensions-0.4.3 pandera-0.9.0 powerlaw-1.5 pyyaml-5.4.1 recommenders-1.1.0 retrying-1.3.3 scikit-surprise-1.1.1 tokenizers-0.12.1 transformers-4.20.1 typing-inspect-0.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQ9CJzf_riRo",
        "outputId": "f60bf6d0-1432-4e15-a20e-a5b313f0cb89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System version: 3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "Tensorflow version: 2.8.2\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from tqdm import tqdm\n",
        "import scrapbook as sb\n",
        "from tempfile import TemporaryDirectory\n",
        "import tensorflow as tf\n",
        "tf.get_logger().setLevel('ERROR') # only show error messages\n",
        "\n",
        "from recommenders.models.deeprec.deeprec_utils import download_deeprec_resources \n",
        "from recommenders.models.newsrec.newsrec_utils import prepare_hparams\n",
        "from recommenders.models.newsrec.models.nrms import NRMSModel\n",
        "from recommenders.models.newsrec.io.mind_iterator import MINDIterator\n",
        "from recommenders.models.newsrec.newsrec_utils import get_mind_data_set\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Tensorflow version: {}\".format(tf.__version__))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZnAp-mxriRq"
      },
      "source": [
        "## Prepare parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "M4_3ABurriRr"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "seed = 1\n",
        "batch_size = 64\n",
        "\n",
        "# Options: demo, small, large\n",
        "MIND_type = 'small'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LG9bAGbriRr"
      },
      "source": [
        "## Download and load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T9Qv1LpriRs",
        "outputId": "365550ab-90ba-4237-8fa8-62aa6ca98f81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51.7k/51.7k [00:09<00:00, 5.74kKB/s]\n",
            "100%|██████████| 30.2k/30.2k [00:35<00:00, 848KB/s]\n",
            "100%|██████████| 152k/152k [00:21<00:00, 6.92kKB/s]\n"
          ]
        }
      ],
      "source": [
        "tmpdir = TemporaryDirectory()\n",
        "data_path = tmpdir.name\n",
        "\n",
        "train_news_file = os.path.join(data_path, 'train', r'news.tsv')\n",
        "train_behaviors_file = os.path.join(data_path, 'train', r'behaviors.tsv')\n",
        "valid_news_file = os.path.join(data_path, 'valid', r'news.tsv')\n",
        "valid_behaviors_file = os.path.join(data_path, 'valid', r'behaviors.tsv')\n",
        "wordEmb_file = os.path.join(data_path, \"utils\", \"embedding.npy\")\n",
        "userDict_file = os.path.join(data_path, \"utils\", \"uid2index.pkl\")\n",
        "wordDict_file = os.path.join(data_path, \"utils\", \"word_dict.pkl\")\n",
        "yaml_file = os.path.join(data_path, \"utils\", r'nrms.yaml')\n",
        "\n",
        "mind_url, mind_train_dataset, mind_dev_dataset, mind_utils = get_mind_data_set(MIND_type)\n",
        "\n",
        "if not os.path.exists(train_news_file):\n",
        "    download_deeprec_resources(mind_url, os.path.join(data_path, 'train'), mind_train_dataset)\n",
        "    \n",
        "if not os.path.exists(valid_news_file):\n",
        "    download_deeprec_resources(mind_url, \\\n",
        "                               os.path.join(data_path, 'valid'), mind_dev_dataset)\n",
        "if not os.path.exists(yaml_file):\n",
        "    download_deeprec_resources(r'https://recodatasets.z20.web.core.windows.net/newsrec/', \\\n",
        "                               os.path.join(data_path, 'utils'), mind_utils)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml8Z_yoIriRt"
      },
      "source": [
        "## Create hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "djmH83g0riRt",
        "outputId": "10198e4c-43e1-4081-f151-1766778ff666"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HParams object with values {'support_quick_scoring': True, 'dropout': 0.2, 'attention_hidden_dim': 200, 'head_num': 20, 'head_dim': 20, 'filter_num': 200, 'window_size': 3, 'vert_emb_dim': 100, 'subvert_emb_dim': 100, 'gru_unit': 400, 'type': 'ini', 'user_emb_dim': 50, 'learning_rate': 0.0001, 'optimizer': 'adam', 'epochs': 10, 'batch_size': 64, 'show_step': 10, 'title_size': 30, 'his_size': 50, 'data_format': 'news', 'npratio': 4, 'metrics': ['group_auc', 'mean_mrr', 'ndcg@5;10'], 'word_emb_dim': 300, 'model_type': 'nrms', 'loss': 'cross_entropy_loss', 'wordEmb_file': '/tmp/tmpeo313ipt/utils/embedding.npy', 'wordDict_file': '/tmp/tmpeo313ipt/utils/word_dict.pkl', 'userDict_file': '/tmp/tmpeo313ipt/utils/uid2index.pkl'}\n"
          ]
        }
      ],
      "source": [
        "hparams = prepare_hparams(yaml_file, \n",
        "                          wordEmb_file=wordEmb_file,\n",
        "                          wordDict_file=wordDict_file, \n",
        "                          userDict_file=userDict_file,\n",
        "                          batch_size=batch_size,\n",
        "                          epochs=epochs,\n",
        "                          show_step=10)\n",
        "print(hparams)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSYJ1tmwriRu"
      },
      "source": [
        "## Train the NRMS model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Tq_LBR65riRu"
      },
      "outputs": [],
      "source": [
        "iterator = MINDIterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltoxzJkQriRu",
        "outputId": "49ac79f3-0fe7-4794-d817-28cdca5da7a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "model = NRMSModel(hparams, iterator, seed=seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tFIs3ni3riRv",
        "outputId": "e9b07794-d8c9-434b-a0ac-8500bd6ac169"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r0it [00:00, ?it/s]/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n",
            "663it [00:04, 140.60it/s]\n",
            "1143it [01:16, 14.94it/s]\n",
            "73152it [00:08, 8413.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'group_auc': 0.4926, 'mean_mrr': 0.2101, 'ndcg@5': 0.2115, 'ndcg@10': 0.2736}\n"
          ]
        }
      ],
      "source": [
        "print(model.run_eval(valid_news_file, valid_behaviors_file))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-qx8C6griRv",
        "outputId": "298a5154-5d24-4f06-dab3-379d99576e76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "step 3690 , total_loss: 1.4105, data_loss: 1.2759: : 3693it [19:43,  3.12it/s]\n",
            "663it [00:01, 533.22it/s]\n",
            "1143it [01:17, 14.75it/s]\n",
            "73152it [00:08, 8485.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at epoch 1\n",
            "train info: logloss loss:1.4104488518642857\n",
            "eval info: group_auc:0.6298, mean_mrr:0.2904, ndcg@10:0.381, ndcg@5:0.3151\n",
            "at epoch 1 , train time: 1183.7 eval time: 147.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "step 3690 , total_loss: 1.3084, data_loss: 1.4897: : 3693it [19:50,  3.10it/s]\n",
            "663it [00:01, 537.28it/s]\n",
            "1143it [01:17, 14.77it/s]\n",
            "73152it [00:08, 8508.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "at epoch 2\n",
            "train info: logloss loss:1.308419421989034\n",
            "eval info: group_auc:0.6374, mean_mrr:0.2966, ndcg@10:0.389, ndcg@5:0.3227\n",
            "at epoch 2 , train time: 1191.0 eval time: 146.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "step 1890 , total_loss: 1.2824, data_loss: 1.4213: : 1897it [10:04,  3.14it/s]"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model.fit(train_news_file, train_behaviors_file, valid_news_file, valid_behaviors_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nt42ZIi9riRv",
        "outputId": "866db222-21c3-4d58-f308-24a096906549"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "586it [00:01, 396.35it/s]\n",
            "236it [00:03, 67.56it/s]\n",
            "7538it [00:01, 6017.89it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'group_auc': 0.6127, 'mean_mrr': 0.2697, 'ndcg@5': 0.2912, 'ndcg@10': 0.3625}\n",
            "CPU times: user 29.8 s, sys: 1.27 s, total: 31.1 s\n",
            "Wall time: 14.3 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "res_syn = model.run_eval(valid_news_file, valid_behaviors_file)\n",
        "print(res_syn)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP7cNjTariRw"
      },
      "outputs": [],
      "source": [
        "sb.glue(\"res_syn\", res_syn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvFwYDt4riRw"
      },
      "source": [
        "## Save the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF1tk7lHriRw"
      },
      "outputs": [],
      "source": [
        "model_path = os.path.join(data_path, \"model\")\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "model.model.save_weights(os.path.join(model_path, \"nrms_ckpt\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KecFNjdriRw"
      },
      "source": [
        "## Output Predcition File\n",
        "This code segment is used to generate the prediction.zip file, which is in the same format in [MIND Competition Submission Tutorial](https://competitions.codalab.org/competitions/24122#learn_the_details-submission-guidelines).\n",
        "\n",
        "Please change the `MIND_type` parameter to `large` if you want to submit your prediction to [MIND Competition](https://msnews.github.io/competition.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bew2klvKriRx",
        "outputId": "47bee20c-a648-40ec-d670-2b5858b97ed9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "586it [00:01, 399.64it/s]\n",
            "236it [00:03, 67.94it/s]\n",
            "7538it [00:00, 8052.34it/s]\n"
          ]
        }
      ],
      "source": [
        "group_impr_indexes, group_labels, group_preds = model.run_fast_eval(valid_news_file, valid_behaviors_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_QCaBN4riRx",
        "outputId": "e07a8378-b9ff-44bd-f3e0-04cb858ce02e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "7538it [00:00, 35200.73it/s]\n"
          ]
        }
      ],
      "source": [
        "with open(os.path.join(data_path, 'prediction.txt'), 'w') as f:\n",
        "    for impr_index, preds in tqdm(zip(group_impr_indexes, group_preds)):\n",
        "        impr_index += 1\n",
        "        pred_rank = (np.argsort(np.argsort(preds)[::-1]) + 1).tolist()\n",
        "        pred_rank = '[' + ','.join([str(i) for i in pred_rank]) + ']'\n",
        "        f.write(' '.join([str(impr_index), pred_rank])+ '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v960vK5riRx"
      },
      "outputs": [],
      "source": [
        "f = zipfile.ZipFile(os.path.join(data_path, 'prediction.zip'), 'w', zipfile.ZIP_DEFLATED)\n",
        "f.write(os.path.join(data_path, 'prediction.txt'), arcname='prediction.txt')\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgYrurJPriRx"
      },
      "source": [
        "## Reference\n",
        "\\[1\\] Wu et al. \"Neural News Recommendation with Multi-Head Self-Attention.\" in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<br>\n",
        "\\[2\\] Wu, Fangzhao, et al. \"MIND: A Large-scale Dataset for News Recommendation\" Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. https://msnews.github.io/competition.html <br>\n",
        "\\[3\\] GloVe: Global Vectors for Word Representation. https://nlp.stanford.edu/projects/glove/"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "interpreter": {
      "hash": "3a9a0c422ff9f08d62211b9648017c63b0a26d2c935edc37ebb8453675d13bb5"
    },
    "kernelspec": {
      "display_name": "Python 3.7.11 64-bit ('tf2': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.11"
    },
    "colab": {
      "name": "nrms_MIND.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}